{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm.sklearn import LGBMRegressor, LGBMClassifier\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_data(path, exclude=[], train=True):\n",
    "    user = {'Alice': 1,'Bob': 2}\n",
    "    if os.path.isdir(path):\n",
    "        data = pd.DataFrame()\n",
    "\n",
    "        for file in os.listdir(path):\n",
    "            if (file.startswith(\"train\") and train or file.startswith(\"test\") and not train) and file not in exclude:\n",
    "                print(\"{} loaded\".format(file))\n",
    "                df = pd.read_json(os.path.join(path, file)).set_index(\"dialogId\")\n",
    "                df['speaker'] = df.thread.apply(lambda x: [user[msg['userId']] for msg in x])\n",
    "                df['thread'] = df.thread.apply(lambda x: [msg['text'] for msg in x], convert_dtype=False)\n",
    "                df['thread_raw'] = df.thread.apply(lambda x: \" \".join(x))\n",
    "                if train:\n",
    "                    df[\"qualA\"] = df.evaluation.apply(lambda x: sorted(x, key=lambda x: x['userId'])[0]['quality'])\n",
    "                    df[\"qualB\"] = df.evaluation.apply(lambda x: sorted(x, key=lambda x: x['userId'])[1]['quality'])\n",
    "                    df[\"botA\"] = df.users.apply(lambda x: sorted(x, key=lambda x: x['id'])[0]['userType'] == 'Bot')\n",
    "                    df[\"botB\"] = df.users.apply(lambda x: sorted(x, key=lambda x: x['id'])[1]['userType'] == 'Bot')\n",
    "                df.drop(['users'], axis=1, inplace=True)\n",
    "                if train:\n",
    "                    df.drop(['evaluation'], axis=1, inplace=True)\n",
    "\n",
    "                data = pd.concat([data, df])\n",
    "    else:\n",
    "        df = pd.read_json(path).set_index(\"dialogId\")\n",
    "        df['speaker'] = df.thread.apply(lambda x: [user[msg['userId']] for msg in x])\n",
    "        df['thread'] = df.thread.apply(lambda x: [msg['text'] for msg in x], convert_dtype=False)\n",
    "        df['thread_raw'] = df.thread.apply(lambda x: \" \".join(x))\n",
    "        if train:\n",
    "            df[\"qualA\"] = df.evaluation.apply(lambda x: sorted(x, key=lambda x: x['userId'])[0]['quality'])\n",
    "            df[\"qualB\"] = df.evaluation.apply(lambda x: sorted(x, key=lambda x: x['userId'])[1]['quality'])\n",
    "            df[\"botA\"] = df.users.apply(lambda x: sorted(x, key=lambda x: x['id'])[0]['userType'] == 'Bot')\n",
    "            df[\"botB\"] = df.users.apply(lambda x: sorted(x, key=lambda x: x['id'])[1]['userType'] == 'Bot')\n",
    "        df.drop(['users'], axis=1, inplace=True)\n",
    "        if train:\n",
    "            df.drop(['evaluation'], axis=1, inplace=True)\n",
    "            \n",
    "        data = df\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/usr/share/dict/words\") as wordfile:\n",
    "    words = set(x.strip().lower() for x in wordfile.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare(data):\n",
    "    features = []\n",
    "    answers = []\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        f = [\n",
    "            [max([0]+[len(list(x)) for x in (g for k, g in itertools.groupby(row[1][\"speaker\"]) if k == 1)])],\n",
    "            [max([0]+[len(list(x)) for x in (g for k, g in itertools.groupby(row[1][\"speaker\"]) if k == 2)])],\n",
    "            [min([0]+[len(list(x)) for x in (g for k, g in itertools.groupby(row[1][\"speaker\"]) if k == 1)])],\n",
    "            [min([0]+[len(list(x)) for x in (g for k, g in itertools.groupby(row[1][\"speaker\"]) if k == 2)])],\n",
    "            [sum(1 for word in row[1][\"thread_A\"].lower().split() if word not in words)],\n",
    "            [sum(1 for word in row[1][\"thread_B\"].lower().split() if word not in words)],\n",
    "            [sum(1 for word in row[1][\"thread_A\"].lower().split() if word in row[1]['context'].lower().split())],\n",
    "            [sum(1 for word in row[1][\"thread_B\"].lower().split() if word in row[1]['context'].lower().split())],\n",
    "            [sum(1 for word in row[1][\"thread_A\"].lower().split() if word not in words) / (1 + len(row[1]['thread_A'].lower().split()))],\n",
    "            [sum(1 for word in row[1][\"thread_B\"].lower().split() if word not in words) / (1 + len(row[1]['thread_B'].lower().split()))],\n",
    "            [sum(1 for word in row[1][\"thread_A\"].lower().split() if word in row[1]['context'].lower().split()) / (1 + len(row[1]['thread_A'].lower().split()))],\n",
    "            [sum(1 for word in row[1][\"thread_B\"].lower().split() if word in row[1]['context'].lower().split()) / (1 + len(row[1]['thread_B'].lower().split()))],\n",
    "            row[1]['counts_A'],\n",
    "            row[1]['counts_B'],\n",
    "            #row[1]['counts_context'],\n",
    "            row[1]['counts_start_A'],\n",
    "            row[1]['counts_start_B'],\n",
    "            #row[1]['tfidf_A'],\n",
    "            #row[1]['tfidf_B'],\n",
    "            #row[1]['tfidf_context'],\n",
    "            #row[1]['tfidf_start_A'],\n",
    "            #row[1]['tfidf_start_B'],\n",
    "        ]\n",
    "        features.append(np.concatenate(f))\n",
    "        \n",
    "        try:\n",
    "            answers.append(row[1][\"qualB\"])\n",
    "            train = True\n",
    "        except:\n",
    "            train = False\n",
    "\n",
    "\n",
    "    features = np.stack(features)\n",
    "    \n",
    "    if train:\n",
    "        answers = np.stack(answers)\n",
    "    \n",
    "    if train:\n",
    "        return (features, answers)\n",
    "    else:\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def spearmancorr(est,X,y):\n",
    "    rho, pval = spearmanr(np.reshape(y, (-1, 1)), np.reshape(est.predict(X), (-1, 1)), axis=0)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def filter_(person, first=False):\n",
    "    def f(row, speaker):\n",
    "        messages = np.array(row['thread'])[np.array(row['speaker']) == speaker]\n",
    "        if first:\n",
    "            messages = messages[:1]\n",
    "        \n",
    "        return \" \".join(messages)\n",
    "\n",
    "    return lambda x: f(x, person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_20170724.json loaded\n",
      "train_20170725.json loaded\n",
      "train_20170726.json loaded\n"
     ]
    }
   ],
   "source": [
    "data = process_data(\"../data/\", train=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data[\"thread_A\"] = data.apply(filter_(1), axis=1)\n",
    "data['thread_B'] = data.apply(filter_(2), axis=1)\n",
    "\n",
    "data[\"start_A\"] = data.apply(filter_(1, True), axis=1)\n",
    "data[\"start_B\"] = data.apply(filter_(2, True), axis=1)\n",
    "\n",
    "tfidf_thread = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1, 2), max_features=2000)\n",
    "data[\"tfidf_all\"] = tfidf_thread.fit_transform(data[\"thread_raw\"]).toarray().tolist()\n",
    "data[\"tfidf_A\"] = tfidf_thread.transform(data[\"thread_A\"]).toarray().tolist()\n",
    "data[\"tfidf_B\"] = tfidf_thread.transform(data[\"thread_B\"]).toarray().tolist()\n",
    "data[\"tfidf_start_A\"] = tfidf_thread.transform(data[\"start_A\"]).toarray().tolist()\n",
    "data[\"tfidf_start_B\"] = tfidf_thread.transform(data[\"start_B\"]).toarray().tolist()\n",
    "\n",
    "tfidf_context = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1, 2), max_features=2000)\n",
    "data[\"tfidf_context\"] = tfidf_context.fit_transform(data[\"context\"]).toarray().tolist()\n",
    "\n",
    "\n",
    "count_thread = CountVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1, 2), max_features=2000)\n",
    "data[\"counts_all\"] = count_thread.fit_transform(data[\"thread_raw\"]).toarray().tolist()\n",
    "data[\"counts_A\"] = count_thread.transform(data[\"thread_A\"]).toarray().tolist()\n",
    "data[\"counts_B\"] = count_thread.transform(data[\"thread_B\"]).toarray().tolist()\n",
    "data[\"counts_start_A\"] = count_thread.transform(data[\"start_A\"]).toarray().tolist()\n",
    "data[\"counts_start_B\"] = count_thread.transform(data[\"start_B\"]).toarray().tolist()\n",
    "\n",
    "count_context = CountVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1, 2), max_features=2000)\n",
    "data[\"counts_context\"] = count_context.fit_transform(data[\"context\"]).toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = prepare(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.552220, total=   1.4s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................. , score=0.634135, total=   1.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    2.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................. , score=0.641359, total=   1.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.632248, total=   1.4s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.529679, total=   1.4s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.591046, total=   1.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.638460, total=   1.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.691395, total=   1.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.476927, total=   1.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.586115, total=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   14.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.59735832417276413, 0.060270969812778144)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = VotingClassifier([('gbm', GradientBoostingClassifier()), ('svc', LinearSVC(tol=0.1)), ('lgbm', LGBMClassifier(n_estimators=100, num_leaves=1000))])\n",
    "\n",
    "#gcv = GridSearchCV(clf, {\"n_neighbors\":[1, 5, 10, 15, 25, 40, 50, 75, 100, 150, 250]}, scoring=spearmancorr, verbose=3)\n",
    "#gcv.fit(X, y)\n",
    "\n",
    "cv = cross_val_score(LGBMRegressor(n_estimators=100, num_leaves=1000), X, y, cv=KFold(10,shuffle=True,random_state=123), verbose=3, scoring=spearmancorr)\n",
    "cv.mean(), cv.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = cross_val_score(LGBMRegressor(n_estimators=100, num_leaves=1000), X, y, cv=10, verbose=3, scoring=spearmancorr)\n",
    "cv.mean(), cv.std()\n",
    "\n",
    "#(0.59851429398955014, 0.060098095791398666)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
