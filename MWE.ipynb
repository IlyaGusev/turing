{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm.sklearn import LGBMRegressor, LGBMClassifier\n",
    "import string\n",
    "\n",
    "import itertools\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/usr/share/dict/words\") as wordfile:\n",
    "    words = set(x.strip().lower() for x in wordfile.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spearmancorr(est,X,y):\n",
    "    rho, pval = spearmanr(np.reshape(y, (-1, 1)), np.reshape(est.predict(X), (-1, 1)), axis=0)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_data(path, exclude=[], train=True):\n",
    "    def process_file(path):\n",
    "        user = {'Alice': \"A\",'Bob': \"B\"}\n",
    "        \n",
    "        df = pd.read_json(path).set_index(\"dialogId\")\n",
    "        df['speaker'] = df[\"thread\"].apply(lambda x: [user[msg['userId']] for msg in x])\n",
    "        df['thread'] = df[\"thread\"].apply(lambda x: [msg['text'] for msg in x], convert_dtype=False)\n",
    "        df['thread_raw'] = df[\"thread\"].apply(lambda x: \" \".join(x))\n",
    "        if train:\n",
    "            df[\"qualA\"] = df[\"evaluation\"].apply(lambda x: sorted(x, key=lambda x: x['userId'])[0]['quality'])\n",
    "            df[\"qualB\"] = df[\"evaluation\"].apply(lambda x: sorted(x, key=lambda x: x['userId'])[1]['quality'])\n",
    "            df[\"botA\"] = df[\"users\"].apply(lambda x: sorted(x, key=lambda x: x['id'])[0]['userType'] == 'Bot')\n",
    "            df[\"botB\"] = df[\"users\"].apply(lambda x: sorted(x, key=lambda x: x['id'])[1]['userType'] == 'Bot')\n",
    "        df.drop(['users'], axis=1, inplace=True)\n",
    "        if train:\n",
    "            df.drop(['evaluation'], axis=1, inplace=True)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def add_features(data):\n",
    "        def preprocess(text, lower, punctuation):\n",
    "            if lower:\n",
    "                text = text.lower()\n",
    "            if punctuation == \"exclude\":\n",
    "                text = text.translate(str.maketrans({p: None for p in string.punctuation}))\n",
    "            elif punctuation == \"separate\":\n",
    "                text = text.translate(str.maketrans({p: \" {} \".format(p) for p in string.punctuation}))\n",
    "\n",
    "            return text\n",
    "\n",
    "        punct_modes = [\"exclude\", \"separate\", \"leave\"]\n",
    "        lower_modes = [True, False]\n",
    "                \n",
    "        for preproc_mode in itertools.product(lower_modes, punct_modes):            \n",
    "            preproc = lambda text: preprocess(text, *preproc_mode)\n",
    "\n",
    "            def get_speaker(speaker):\n",
    "                return lambda row: [preproc(x) for x in np.array(row['thread'])[np.array(row['speaker']) == speaker]]\n",
    "\n",
    "            data[\"thread_split_A_{}_{}\".format(*preproc_mode)] = data.apply(get_speaker(\"A\"), axis=1)\n",
    "            data['thread_split_B_{}_{}'.format(*preproc_mode)] = data.apply(get_speaker(\"B\"), axis=1)\n",
    "            \n",
    "            def join_speaker(speaker):\n",
    "                return lambda row: \" \".join(row[\"thread_split_{}_{}_{}\".format(speaker, *preproc_mode)])\n",
    "            \n",
    "            data[\"thread_joined_A_{}_{}\".format(*preproc_mode)] = data.apply(join_speaker(\"A\"), axis=1)\n",
    "            data[\"thread_joined_B_{}_{}\".format(*preproc_mode)] = data.apply(join_speaker(\"B\"), axis=1)\n",
    "\n",
    "            def get_first(speaker):\n",
    "                return lambda row: \" \".join(row[\"thread_split_{}_{}_{}\".format(speaker, *preproc_mode)])\n",
    "            \n",
    "            data[\"start_A_{}_{}\".format(*preproc_mode)] = data.apply(get_first(\"A\"), axis=1)\n",
    "            data[\"start_B_{}_{}\".format(*preproc_mode)] = data.apply(get_first(\"B\"), axis=1)\n",
    "\n",
    "            tfidf_thread = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), max_features=4000)\n",
    "            \n",
    "            data[\"tfidf_all_{}_{}\".format(*preproc_mode)] = tfidf_thread.fit_transform(data[\"thread_raw\"]).toarray().tolist()\n",
    "            data[\"tfidf_A_{}_{}\".format(*preproc_mode)] = tfidf_thread.transform(data[\"thread_joined_A_{}_{}\".format(*preproc_mode)]).toarray().tolist()\n",
    "            data[\"tfidf_B_{}_{}\".format(*preproc_mode)] = tfidf_thread.transform(data[\"thread_joined_B_{}_{}\".format(*preproc_mode)]).toarray().tolist()\n",
    "            data[\"tfidf_start_A_{}_{}\".format(*preproc_mode)] = tfidf_thread.transform(data[\"start_A_{}_{}\".format(*preproc_mode)]).toarray().tolist()\n",
    "            data[\"tfidf_start_B_{}_{}\".format(*preproc_mode)] = tfidf_thread.transform(data[\"start_B_{}_{}\".format(*preproc_mode)]).toarray().tolist()\n",
    "\n",
    "            tfidf_context = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), max_features=4000)\n",
    "            data[\"tfidf_context_{}_{}\".format(*preproc_mode)] = tfidf_context.fit_transform(data[\"context\"]).toarray().tolist()\n",
    "\n",
    "            count_thread = CountVectorizer(analyzer='word', ngram_range=(1, 2), max_features=4000)\n",
    "            data[\"counts_all_{}_{}\".format(*preproc_mode)] = count_thread.fit_transform(data[\"thread_raw\"]).toarray().tolist()\n",
    "            data[\"counts_A_{}_{}\".format(*preproc_mode)] = count_thread.transform(data[\"thread_joined_A_{}_{}\".format(*preproc_mode)]).toarray().tolist()\n",
    "            data[\"counts_B_{}_{}\".format(*preproc_mode)] = count_thread.transform(data[\"thread_joined_B_{}_{}\".format(*preproc_mode)]).toarray().tolist()\n",
    "            data[\"counts_start_A_{}_{}\".format(*preproc_mode)] = count_thread.transform(data[\"start_A_{}_{}\".format(*preproc_mode)]).toarray().tolist()\n",
    "            data[\"counts_start_B_{}_{}\".format(*preproc_mode)] = count_thread.transform(data[\"start_B_{}_{}\".format(*preproc_mode)]).toarray().tolist()\n",
    "\n",
    "            count_context = CountVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1, 2), max_features=4000)\n",
    "            data[\"counts_context_{}_{}\".format(*preproc_mode)] = count_context.fit_transform(data[\"context\"]).toarray().tolist()\n",
    "            \n",
    "            def run_len(target, func):\n",
    "                return lambda row: [func((len(list(g)) for person, g in itertools.groupby(row[\"speaker\"]) if person == target), default=0)]\n",
    "\n",
    "            data[\"f_max_run_A_{}_{}\".format(*preproc_mode)] = data.apply(run_len(\"A\", max), axis=1)\n",
    "            data[\"f_max_run_B_{}_{}\".format(*preproc_mode)] = data.apply(run_len(\"B\", max), axis=1)\n",
    "            data[\"f_min_run_A_{}_{}\".format(*preproc_mode)] = data.apply(run_len(\"A\", min), axis=1)\n",
    "            data[\"f_min_run_B_{}_{}\".format(*preproc_mode)] = data.apply(run_len(\"B\", min), axis=1)\n",
    "\n",
    "            def typo_count(target):\n",
    "                return lambda row: [sum(1 for word in preproc(row[\"thread_joined_{}_{}_{}\".format(target, *preproc_mode)]) if word not in words)]\n",
    "\n",
    "            data[\"f_typos_A_{}_{}\".format(*preproc_mode)] = data.apply(typo_count(\"A\"), axis=1)\n",
    "            data[\"f_typos_B_{}_{}\".format(*preproc_mode)] = data.apply(typo_count(\"B\"), axis=1)\n",
    "            data[\"f_typos_frac_A_{}_{}\".format(*preproc_mode)] = data.apply(lambda row: row[\"f_typos_A_{}_{}\".format(*preproc_mode)][0] / (1 + len(preproc(row[\"thread_joined_A_{}_{}\".format(*preproc_mode)]).split())), axis=1)\n",
    "            data[\"f_typos_frac_B_{}_{}\".format(*preproc_mode)] = data.apply(lambda row: row[\"f_typos_B_{}_{}\".format(*preproc_mode)][0] / (1 + len(preproc(row[\"thread_joined_B_{}_{}\".format(*preproc_mode)]).split())), axis=1)\n",
    "\n",
    "            \n",
    "            def relevant_words(target):\n",
    "                return lambda row: [sum(1 for word in preproc(row[\"thread_joined_{}_{}_{}\".format(target, *preproc_mode)]) if word in preproc(row['context']))]\n",
    "        \n",
    "            data[\"f_relevant_A_{}_{}\".format(*preproc_mode)] = data.apply(relevant_words(\"A\"), axis=1)\n",
    "            data[\"f_relevant_B_{}_{}\".format(*preproc_mode)] = data.apply(relevant_words(\"B\"), axis=1)\n",
    "            \n",
    "            print(\"!\")\n",
    "        return data\n",
    "            \n",
    "    if os.path.isdir(path):\n",
    "        data = pd.concat(\n",
    "            [\n",
    "                process_file(os.path.join(path, file))\n",
    "                for file in os.listdir(path)\n",
    "                if (\n",
    "                    file.startswith(\"train\") and train or\n",
    "                    file.startswith(\"test\") and not train\n",
    "                ) and file not in exclude\n",
    "            ]\n",
    "        )\n",
    "    else:            \n",
    "        data = process_file(path)\n",
    "        \n",
    "    data = add_features(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:38: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "data = process_data(\"../data/\", train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X =((data[['tfidf_all_True_exclude',\n",
    " 'tfidf_A_True_exclude',\n",
    " 'tfidf_B_True_exclude',\n",
    " 'tfidf_start_A_True_exclude',\n",
    " 'tfidf_start_B_True_exclude',\n",
    " 'tfidf_context_True_exclude',\n",
    " 'counts_all_True_exclude',\n",
    " 'counts_A_True_exclude',\n",
    " 'counts_B_True_exclude',\n",
    " 'counts_start_A_True_exclude',\n",
    " 'counts_start_B_True_exclude',\n",
    " 'counts_context_True_exclude',\n",
    " 'f_max_run_A_True_exclude',\n",
    " 'f_max_run_B_True_exclude',\n",
    " 'f_min_run_A_True_exclude',\n",
    " 'f_min_run_B_True_exclude',\n",
    " 'f_typos_A_True_exclude',\n",
    " 'f_typos_B_True_exclude',\n",
    " 'f_relevant_A_True_exclude',\n",
    " 'f_relevant_B_True_exclude',]]).values)\n",
    "X = np.stack([np.concatenate(X[i]) for i in range(X.shape[0])])\n",
    "\n",
    "y = data[\"qualA\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', colsample_bytree=1, drop_rate=0.1,\n",
       "       fair_c=1.0, gaussian_eta=1.0, huber_delta=1.0, learning_rate=0.1,\n",
       "       max_bin=255, max_depth=-1, max_drop=50, min_child_samples=10,\n",
       "       min_child_weight=5, min_split_gain=0, n_estimators=100, nthread=-1,\n",
       "       num_leaves=1000, objective='regression', poisson_max_delta_step=0.7,\n",
       "       reg_alpha=0, reg_lambda=0, seed=0, silent=True, skip_drop=0.5,\n",
       "       subsample=1, subsample_for_bin=50000, subsample_freq=1,\n",
       "       uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBMRegressor(n_estimators=100, num_leaves=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = cross_val_score(LGBMRegressor(n_estimators=100, num_leaves=1000), X, y, cv=10, verbose=3, scoring=spearmancorr)\n",
    "cv.mean(), cv.std()\n",
    "\n",
    "#(0.59851429398955014, 0.060098095791398666)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
