{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import LSTM, Bidirectional, Dropout, Dense, Input, Embedding\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = \"data\"\n",
    "TRAIN_FILENAME = os.path.join(DIR, \"train.txt\")\n",
    "TEST_FILENAME = os.path.join(DIR, \"test.txt\")\n",
    "VAL_FILENAME = os.path.join(DIR, \"validation.txt\")\n",
    "MODEL_FILENAME = os.path.join(\"models\", \"LSTM256_Emb30_Dense128_dropout0.3.h5\")\n",
    "VOCABULARY_PATH = \"vocabulary.pickle\"\n",
    "W2V_PATH= \"resources/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sample = namedtuple('Sample', 'id context response answer')\n",
    "ExtendedSample = namedtuple('ExtendedSample', 'id tweets answer')\n",
    "Tweet = namedtuple('Tweet', 'speaker text')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"@@ \", \"\")\n",
    "    text = text.replace(\"<at>\", \"\")\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = text.lower().split(\" \")\n",
    "    return \" \".join(text)\n",
    "\n",
    "def get_tweets(text):\n",
    "    borders = []\n",
    "    elements = [\"<first_speaker>\", \"<second_speaker>\", \"<minor_speaker>\", \"<third_speaker>\"]\n",
    "    for element in elements:\n",
    "        borders += [m.start() for m in re.finditer(element, text)]\n",
    "    borders.append(len(text))\n",
    "    borders = list(sorted(borders))\n",
    "    tweets = [text[borders[i]:borders[i+1]] for i in range(len(borders)-1)]\n",
    "    for i, sentence in enumerate(tweets):\n",
    "        for key in elements:\n",
    "            if key in sentence:\n",
    "                tweets[i] = Tweet(text=sentence.replace(key, \"\"), speaker=key)\n",
    "    return tweets\n",
    "\n",
    "def samples(filename):\n",
    "    reader = csv.reader(open(filename, \"r\", encoding=\"utf-8\"), delimiter='\\t')\n",
    "    header = next(reader)\n",
    "    for sample in reader:\n",
    "        if len(header) == 3:\n",
    "            sample.append(None)\n",
    "        sample = Sample._make(sample)  # type: Sample\n",
    "        sample = Sample(id=sample.id,\n",
    "                        context=clean_text(sample.context),\n",
    "                        response=clean_text(sample.response),\n",
    "                        answer=sample.answer)\n",
    "        yield ExtendedSample(id=sample.id,\n",
    "                             tweets=get_tweets(sample.context) + get_tweets(sample.response),\n",
    "                             answer=sample.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100001\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Индексированный словарь.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dump_filename):\n",
    "        self.dump_filename = dump_filename\n",
    "        self.reset()\n",
    "\n",
    "        if os.path.isfile(self.dump_filename):\n",
    "            self.load()\n",
    "\n",
    "    def save(self) -> None:\n",
    "        with open(self.dump_filename, \"wb\") as f:\n",
    "            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.dump_filename, \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "            self.__dict__.update(vocab.__dict__)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if self.word_to_index.get(word) is None:\n",
    "            self.index_to_word.append(word)\n",
    "            index = len(self.index_to_word) - 1\n",
    "            self.word_to_index[word] = index\n",
    "            self.count_word(word)\n",
    "            return index\n",
    "        return self.word_to_index[word]\n",
    "\n",
    "    def count_word(self, word):\n",
    "        self.counter[word] += 1\n",
    "\n",
    "    def get_word_index(self, word) -> int:\n",
    "        if self.word_to_index.get(word) is not None:\n",
    "            return self.word_to_index[word]\n",
    "        return -1\n",
    "\n",
    "    def get_word(self, index):\n",
    "        return self.index_to_word[index]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.index_to_word)\n",
    "\n",
    "    def reset(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = []\n",
    "        self.counter = Counter()\n",
    "        self.word_to_index[\"NotAWord\"] = 0\n",
    "        self.index_to_word.append(\"NotAWord\")\n",
    "        self.counter[\"NotAWord\"] = 1\n",
    "\n",
    "    def shrink(self, num):\n",
    "        pairs = self.counter.most_common(num)\n",
    "        self.reset()\n",
    "        for word, count in pairs:\n",
    "            self.add_word(word)\n",
    "def collect_vocabulary(vocabulary, filename):\n",
    "    i = 0\n",
    "    for sample in samples(filename):\n",
    "        i += 1\n",
    "        for tweet in sample.tweets:\n",
    "            for word in tweet.text.split():\n",
    "                vocabulary.add_word(word)\n",
    "        if i%100000 == 0:\n",
    "            print(i)\n",
    "\n",
    "vocabulary = Vocabulary(VOCABULARY_PATH)\n",
    "if vocabulary.size() <= 1:\n",
    "    collect_vocabulary(vocabulary, TEST_FILENAME)\n",
    "    collect_vocabulary(vocabulary, TRAIN_FILENAME)\n",
    "    collect_vocabulary(vocabulary, VAL_FILENAME)\n",
    "    vocabulary.save()\n",
    "    print(vocabulary.size())\n",
    "vocabulary.shrink(100000)\n",
    "print(vocabulary.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_indices(text, vocabulary):\n",
    "    indices = []\n",
    "    for word in text.split():\n",
    "        index = vocabulary.get_word_index(word)\n",
    "        indices.append(index if index != -1 else vocabulary.size())\n",
    "    return indices\n",
    "\n",
    "\n",
    "def collect_data(filename, vocabulary, n=None, maxlen=100):\n",
    "    if n is None:\n",
    "        n = sum(1 for line in open(filename))\n",
    "    data = []\n",
    "    labels = []\n",
    "    answers = []\n",
    "    i = 1\n",
    "    for sample in samples(filename):\n",
    "        word_indices = []\n",
    "        speaker_labels = []\n",
    "        response_speaker = sample.tweets[-1].speaker\n",
    "#         all_speakers = set([tweet.speaker for tweet in sample.tweets])\n",
    "#         if len(all_speakers) != 2:\n",
    "#             continue\n",
    "        for tweet in sample.tweets:\n",
    "            word_indices += text_to_indices(tweet.text, vocabulary)\n",
    "            for word in tweet.text.split():\n",
    "                speaker_labels.append(float(tweet.speaker == response_speaker))\n",
    "        answer = sample.answer\n",
    "        word_indices = np.array(word_indices, dtype=\"int32\")\n",
    "        speaker_labels = np.array(speaker_labels, dtype=\"float32\")\n",
    "        data.append(word_indices)\n",
    "        labels.append(speaker_labels)\n",
    "        answers.append(answer)\n",
    "        if i == n:\n",
    "            data = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "            labels = sequence.pad_sequences(labels, maxlen=maxlen)\n",
    "            labels = labels.reshape(labels.shape[0], labels.shape[1], 1)\n",
    "            yield (data, labels, answers)\n",
    "            data = []\n",
    "            labels = []\n",
    "            answers = []\n",
    "            i = 0\n",
    "        i += 1\n",
    "    data = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "    labels = sequence.pad_sequences(labels, maxlen=maxlen)\n",
    "    labels = labels.reshape(labels.shape[0], labels.shape[1], 1)\n",
    "    yield (data, labels, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2v(embeddings_filename):\n",
    "    w2v = KeyedVectors.load_word2vec_format(embeddings_filename, binary=True)\n",
    "    return w2v\n",
    "\n",
    "def get_weights(w2v, vocabulary, embedding_dim=300):\n",
    "    weights = np.random.uniform(low=-0.1, high=0.1, size=(vocabulary.size() + 1, embedding_dim))\n",
    "    weights[0] = np.zeros((embedding_dim))\n",
    "    for i, word in enumerate(vocabulary.index_to_word):\n",
    "        if word in w2v.vocab:\n",
    "            weights[i] = w2v.word_vec(word)\n",
    "    return weights\n",
    "\n",
    "w2v = load_w2v(W2V_PATH)\n",
    "weights = get_weights(w2v, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TuringRNN:\n",
    "    def __init__(self, rnn=LSTM, units_rnn=256, units_dense=128, dropout=0.3, batch_size=128, emb_size=300,\n",
    "                 maxlen=100):\n",
    "        self.rnn = rnn\n",
    "        self.batch_size = batch_size\n",
    "        self.units_rnn = units_rnn\n",
    "        self.units_dense = units_dense\n",
    "        self.dropout = dropout\n",
    "        self.model = None\n",
    "        self.maxlen = maxlen\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def build(self, vocabulary, weigths) -> None:\n",
    "        \"\"\"\n",
    "        Построение модели.\n",
    "        \"\"\"\n",
    "        word_input = Input(shape=(self.maxlen,), dtype='int32')\n",
    "        speaker_labels = Input(shape=(self.maxlen, 1), dtype='float32')\n",
    "        word_emb = Embedding(weights.shape[0], weights.shape[1], weights=[weights, ], trainable=False)(word_input)\n",
    "        emb = concatenate([word_emb, speaker_labels], axis=-1)\n",
    "        encoded = Bidirectional(self.rnn(self.units_rnn, recurrent_dropout=0.3))(emb)\n",
    "        merged = Dropout(self.dropout)(encoded)\n",
    "\n",
    "        dense1 = Dense(self.units_dense, activation='relu')(merged)\n",
    "        dense1 = Dropout(self.dropout)(dense1)\n",
    "\n",
    "        predictions = Dense(1, activation='sigmoid')(dense1)\n",
    "        model = Model(inputs=[word_input, speaker_labels], outputs=predictions)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, train_filename, val_filename, vocabulary) -> None:\n",
    "        \"\"\"\n",
    "        Обучение модели.\n",
    "        \"\"\"\n",
    "        x_val_context, x_val_response, y_val = next(collect_data(val_filename, vocabulary, None))\n",
    "        for i in range(50):\n",
    "            j = 0\n",
    "            for x_train_context, x_train_response, y_train in collect_data(train_filename, vocabulary, 100000):\n",
    "                filename = \"{rnn}{units_rnn}_Emb{emb}_Dense{units_dense}_dropout{dropout}.h5\"\n",
    "                filename = filename.format(rnn=self.rnn.__name__, units_rnn=self.units_rnn,\n",
    "                                           units_dense=self.units_dense, dropout=self.dropout,\n",
    "                                           emb=self.emb_size)\n",
    "                filename = os.path.join(os.getcwd(), \"models\", filename)\n",
    "                print(\"Big epoch: \", i, j)\n",
    "                self.model.fit([x_train_context, x_train_response], y_train,\n",
    "                               epochs=1,\n",
    "                               batch_size=self.batch_size,\n",
    "                               shuffle=True,\n",
    "                               verbose=1)\n",
    "                self.model.save(filename)\n",
    "                j += 1\n",
    "            self.model.evaluate([x_val_context, x_val_response], y_val, batch_size=self.batch_size)\n",
    "\n",
    "    def load(self, filename: str) -> None:\n",
    "        self.model = load_model(filename)\n",
    "\n",
    "    def predict(self, x, x_labels):\n",
    "        preds = self.model.predict([x, x_labels], batch_size=self.batch_size, verbose=1)\n",
    "        submission = pd.DataFrame({'id': ids, 'Bob': preds.ravel()})\n",
    "        submission.to_csv(os.path.join(os.getcwd(), 'answer.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = TuringRNN(batch_size=256)\n",
    "rnn.build(vocabulary, weights)\n",
    "rnn.train(TRAIN_FILENAME, VAL_FILENAME, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class Dialog:\n",
    "    def __init__(self, dialog_id, context, first_user_id, second_user_id,\n",
    "                 first_user_is_bot=None, second_user_is_bot=None):\n",
    "        self.dialog_id = int(dialog_id)\n",
    "        self.first_user_id = first_user_id\n",
    "        self.second_user_id = second_user_id\n",
    "        self.context = context\n",
    "        self.messages = []\n",
    "        self.first_user_is_bot = first_user_is_bot\n",
    "        self.second_user_is_bot = second_user_is_bot\n",
    "\n",
    "    def add_message(self, user_id, text):\n",
    "        Message = namedtuple(\"Message\", \"user_id text\")\n",
    "        self.messages.append(Message(user_id, text))\n",
    "\n",
    "    def get_first_user_messages(self):\n",
    "        return [message.text for message in self.messages if message.user_id == self.first_user_id]\n",
    "\n",
    "    def get_second_user_messages(self):\n",
    "        return [message.text for message in self.messages if message.user_id == self.second_user_id]\n",
    "\n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "\n",
    "    def get_context(self):\n",
    "        return self.context\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.dialog_id) + \" \" + self.first_user_id + \" \" + self.second_user_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def parse(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        dialogs = []\n",
    "        if text[0] == \"[\":\n",
    "            text = '{\"dialogs\": ' + text + '}'\n",
    "            dialogs = json.loads(text)[\"dialogs\"]\n",
    "        else:\n",
    "            dialogs.append(json.loads(text))\n",
    "\n",
    "        result = []\n",
    "        for dialog in dialogs:\n",
    "            users = dialog[\"users\"]\n",
    "            messages = dialog[\"thread\"]\n",
    "            first_user = users[0]\n",
    "            second_user = users[1]\n",
    "            first_user_is_bot = None\n",
    "            second_user_is_bot = None\n",
    "            if \"userType\" in first_user:\n",
    "                first_user_is_bot = first_user[\"userType\"] != \"Human\"\n",
    "                second_user_is_bot = second_user[\"userType\"] != \"Human\"\n",
    "\n",
    "            dialog = Dialog(dialog_id=dialog[\"dialogId\"], context=dialog[\"context\"],\n",
    "                            first_user_id=first_user[\"id\"], second_user_id=second_user[\"id\"],\n",
    "                            first_user_is_bot=first_user_is_bot, second_user_is_bot=second_user_is_bot)\n",
    "            for message in messages:\n",
    "                dialog.add_message(message[\"userId\"], message[\"text\"])\n",
    "            result.append(dialog)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect(json_filename, maxlen=100):\n",
    "    dialogs = parse(json_filename)\n",
    "    data = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    for dialog in dialogs:\n",
    "        word_indices = []\n",
    "        speaker_labels = []\n",
    "        texts = [message.text for message in dialog.messages]\n",
    "        for text in texts:\n",
    "            word_indices += text_to_indices(text, vocabulary)\n",
    "        speaker_labels += [message.user_id == \"Bob\" for message in dialog.messages]\n",
    "        word_indices = np.array(word_indices, dtype=\"int32\")\n",
    "        speaker_labels = np.array(speaker_labels, dtype=\"float32\")\n",
    "        data.append(word_indices)\n",
    "        labels.append(speaker_labels)\n",
    "    data = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "    labels = sequence.pad_sequences(labels, maxlen=maxlen)\n",
    "    labels = labels.reshape(labels.shape[0], labels.shape[1], 1)\n",
    "    return (data, labels)\n",
    "\n",
    "rnn = TuringRNN(batch_size=256)\n",
    "rnn.load(\"models/LSTM256_Emb300_Dense128_dropout0.3.h5\")\n",
    "rnn.predict(collect(\"data/day0.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
