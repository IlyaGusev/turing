{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import LSTM, Bidirectional, Dropout, Dense, Input, Embedding\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = \"data\"\n",
    "TRAIN_FILENAME = os.path.join(DIR, \"train.txt\")\n",
    "TEST_FILENAME = os.path.join(DIR, \"test.txt\")\n",
    "VAL_FILENAME = os.path.join(DIR, \"validation.txt\")\n",
    "MODEL_FILENAME = os.path.join(\"models\", \"LSTM256_Emb30_Dense128_dropout0.3.h5\")\n",
    "VOCABULARY_PATH = \"vocabulary.pickle\"\n",
    "W2V_PATH= \"resources/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sample = namedtuple('Sample', 'id context response answer')\n",
    "ExtendedSample = namedtuple('ExtendedSample', 'id tweets answer')\n",
    "Tweet = namedtuple('Tweet', 'speaker text')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"@@ \", \"\")\n",
    "    text = text.replace(\"<at>\", \"\")\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = text.lower().split(\" \")\n",
    "    return \" \".join(text)\n",
    "\n",
    "def get_tweets(text):\n",
    "    borders = []\n",
    "    elements = [\"<first_speaker>\", \"<second_speaker>\", \"<minor_speaker>\", \"<third_speaker>\"]\n",
    "    for element in elements:\n",
    "        borders += [m.start() for m in re.finditer(element, text)]\n",
    "    borders.append(len(text))\n",
    "    borders = list(sorted(borders))\n",
    "    tweets = [text[borders[i]:borders[i+1]] for i in range(len(borders)-1)]\n",
    "    for i, sentence in enumerate(tweets):\n",
    "        for key in elements:\n",
    "            if key in sentence:\n",
    "                tweets[i] = Tweet(text=sentence.replace(key, \"\"), speaker=key)\n",
    "    return tweets\n",
    "\n",
    "def samples(filename):\n",
    "    reader = csv.reader(open(filename, \"r\", encoding=\"utf-8\"), delimiter='\\t')\n",
    "    header = next(reader)\n",
    "    for sample in reader:\n",
    "        if len(header) == 3:\n",
    "            sample.append(None)\n",
    "        sample = Sample._make(sample)  # type: Sample\n",
    "        sample = Sample(id=sample.id,\n",
    "                        context=clean_text(sample.context),\n",
    "                        response=clean_text(sample.response),\n",
    "                        answer=sample.answer)\n",
    "        yield ExtendedSample(id=sample.id,\n",
    "                             tweets=get_tweets(sample.context) + get_tweets(sample.response),\n",
    "                             answer=sample.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "796534\n",
      "100001\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Индексированный словарь.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dump_filename):\n",
    "        self.dump_filename = dump_filename\n",
    "        self.reset()\n",
    "\n",
    "        if os.path.isfile(self.dump_filename):\n",
    "            self.load()\n",
    "\n",
    "    def save(self) -> None:\n",
    "        with open(self.dump_filename, \"wb\") as f:\n",
    "            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.dump_filename, \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "            self.__dict__.update(vocab.__dict__)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if self.word_to_index.get(word) is None:\n",
    "            self.index_to_word.append(word)\n",
    "            index = len(self.index_to_word) - 1\n",
    "            self.word_to_index[word] = index\n",
    "            self.count_word(word)\n",
    "            return index\n",
    "        return self.word_to_index[word]\n",
    "\n",
    "    def count_word(self, word):\n",
    "        self.counter[word] += 1\n",
    "\n",
    "    def get_word_index(self, word) -> int:\n",
    "        if self.word_to_index.get(word) is not None:\n",
    "            return self.word_to_index[word]\n",
    "        return -1\n",
    "\n",
    "    def get_word(self, index):\n",
    "        return self.index_to_word[index]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.index_to_word)\n",
    "\n",
    "    def reset(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = []\n",
    "        self.counter = Counter()\n",
    "        self.word_to_index[\"NotAWord\"] = 0\n",
    "        self.index_to_word.append(\"NotAWord\")\n",
    "        self.counter[\"NotAWord\"] = 1\n",
    "\n",
    "    def shrink(self, num):\n",
    "        pairs = self.counter.most_common(num)\n",
    "        self.reset()\n",
    "        for word, count in pairs:\n",
    "            self.add_word(word)\n",
    "def collect_vocabulary(vocabulary, filename):\n",
    "    i = 0\n",
    "    for sample in samples(filename):\n",
    "        i += 1\n",
    "        for tweet in sample.tweets:\n",
    "            for word in tweet.text.split():\n",
    "                vocabulary.add_word(word)\n",
    "        if i%100000 == 0:\n",
    "            print(i)\n",
    "\n",
    "vocabulary = Vocabulary(VOCABULARY_PATH)\n",
    "if vocabulary.size() <= 1:\n",
    "    collect_vocabulary(vocabulary, TEST_FILENAME)\n",
    "    collect_vocabulary(vocabulary, TRAIN_FILENAME)\n",
    "    collect_vocabulary(vocabulary, VAL_FILENAME)\n",
    "    vocabulary.save()\n",
    "    print(vocabulary.size())\n",
    "vocabulary.shrink(100000)\n",
    "print(vocabulary.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_indices(text, vocabulary):\n",
    "    indices = []\n",
    "    for word in text.split():\n",
    "        index = vocabulary.get_word_index(word)\n",
    "        indices.append(index if index != -1 else vocabulary.size())\n",
    "    return indices\n",
    "\n",
    "\n",
    "def collect_data(filename, vocabulary, n=None, maxlen=100):\n",
    "    if n is None:\n",
    "        n = sum(1 for line in open(filename))\n",
    "    data = []\n",
    "    labels = []\n",
    "    answers = []\n",
    "    i = 1\n",
    "    for sample in samples(filename):\n",
    "        word_indices = []\n",
    "        speaker_labels = []\n",
    "        response_speaker = sample.tweets[-1].speaker\n",
    "#         all_speakers = set([tweet.speaker for tweet in sample.tweets])\n",
    "#         if len(all_speakers) != 2:\n",
    "#             continue\n",
    "        for tweet in sample.tweets:\n",
    "            word_indices += text_to_indices(tweet.text, vocabulary)\n",
    "            for word in tweet.text.split():\n",
    "                speaker_labels.append(float(tweet.speaker == response_speaker))\n",
    "        answer = sample.answer\n",
    "        word_indices = np.array(word_indices, dtype=\"int32\")\n",
    "        speaker_labels = np.array(speaker_labels, dtype=\"float32\")\n",
    "        data.append(word_indices)\n",
    "        labels.append(speaker_labels)\n",
    "        answers.append(answer)\n",
    "        if i == n:\n",
    "            data = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "            labels = sequence.pad_sequences(labels, maxlen=maxlen)\n",
    "            labels = labels.reshape(labels.shape[0], labels.shape[1], 1)\n",
    "            yield (data, labels, answers)\n",
    "            data = []\n",
    "            labels = []\n",
    "            answers = []\n",
    "            i = 0\n",
    "        i += 1\n",
    "    data = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "    labels = sequence.pad_sequences(labels, maxlen=maxlen)\n",
    "    labels = labels.reshape(labels.shape[0], labels.shape[1], 1)\n",
    "    yield (data, labels, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_w2v(embeddings_filename):\n",
    "    w2v = KeyedVectors.load_word2vec_format(embeddings_filename, binary=True)\n",
    "    return w2v\n",
    "\n",
    "def get_weights(w2v, vocabulary, embedding_dim=300):\n",
    "    weights = np.random.uniform(low=-0.1, high=0.1, size=(vocabulary.size() + 1, embedding_dim))\n",
    "    weights[0] = np.zeros((embedding_dim))\n",
    "    for i, word in enumerate(vocabulary.index_to_word):\n",
    "        if word in w2v.vocab:\n",
    "            weights[i] = w2v.word_vec(word)\n",
    "    return weights\n",
    "\n",
    "w2v = load_w2v(W2V_PATH)\n",
    "weights = get_weights(w2v, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TuringRNN:\n",
    "    def __init__(self, rnn=LSTM, units_rnn=256, units_dense=128, dropout=0.3, batch_size=128, emb_size=300,\n",
    "                 maxlen=100):\n",
    "        self.rnn = rnn\n",
    "        self.batch_size = batch_size\n",
    "        self.units_rnn = units_rnn\n",
    "        self.units_dense = units_dense\n",
    "        self.dropout = dropout\n",
    "        self.model = None\n",
    "        self.maxlen = maxlen\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def build(self, vocabulary, weigths) -> None:\n",
    "        \"\"\"\n",
    "        Построение модели.\n",
    "        \"\"\"\n",
    "        word_input = Input(shape=(self.maxlen,), dtype='int32')\n",
    "        speaker_labels = Input(shape=(self.maxlen, 1), dtype='float32')\n",
    "        word_emb = Embedding(weights.shape[0], weights.shape[1], weights=[weights, ], trainable=False)(word_input)\n",
    "        emb = concatenate([word_emb, speaker_labels], axis=-1)\n",
    "        encoded = Bidirectional(self.rnn(self.units_rnn, recurrent_dropout=0.3))(emb)\n",
    "        merged = Dropout(self.dropout)(encoded)\n",
    "\n",
    "        dense1 = Dense(self.units_dense, activation='relu')(merged)\n",
    "        dense1 = Dropout(self.dropout)(dense1)\n",
    "\n",
    "        predictions = Dense(1, activation='sigmoid')(dense1)\n",
    "        model = Model(inputs=[word_input, speaker_labels], outputs=predictions)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, train_filename, val_filename, vocabulary) -> None:\n",
    "        \"\"\"\n",
    "        Обучение модели.\n",
    "        \"\"\"\n",
    "        x_val_context, x_val_response, y_val = next(collect_data(val_filename, vocabulary, None))\n",
    "        for i in range(50):\n",
    "            j = 0\n",
    "            for x_train_context, x_train_response, y_train in collect_data(train_filename, vocabulary, 100000):\n",
    "                filename = \"{rnn}{units_rnn}_Emb{emb}_Dense{units_dense}_dropout{dropout}.h5\"\n",
    "                filename = filename.format(rnn=self.rnn.__name__, units_rnn=self.units_rnn,\n",
    "                                           units_dense=self.units_dense, dropout=self.dropout,\n",
    "                                           emb=self.emb_size)\n",
    "                filename = os.path.join(os.getcwd(), \"models\", filename)\n",
    "                print(\"Big epoch: \", i, j)\n",
    "                self.model.fit([x_train_context, x_train_response], y_train,\n",
    "                               epochs=1,\n",
    "                               batch_size=self.batch_size,\n",
    "                               shuffle=True,\n",
    "                               verbose=1)\n",
    "                self.model.save(filename)\n",
    "                j += 1\n",
    "            self.model.evaluate([x_val_context, x_val_response], y_val, batch_size=self.batch_size)\n",
    "\n",
    "    def load(self, filename: str) -> None:\n",
    "        self.model = load_model(filename)\n",
    "\n",
    "    def predict(self, test_filename, vocabulary):\n",
    "        x_test_words, x_test_labels, _ = next(collect_data(test_filename, vocabulary, None))\n",
    "        ids = [sample.id for sample in samples(test_filename)]\n",
    "        preds = self.model.predict([x_test_words, x_test_labels], batch_size=self.batch_size, verbose=1)\n",
    "        submission = pd.DataFrame({'id': ids, 'human-generated': preds.ravel()})\n",
    "        submission.to_csv(os.path.join(os.getcwd(), 'submitions', 'answer.csv'), index=False)\n",
    "    \n",
    "    def predict_new(self, collect, test_filename, vocabulary):\n",
    "        dialogs = parse(test_filename)\n",
    "        ids = [dialog.dialog_id for dialog in dialogs]\n",
    "        x_test_words, x_test_labels = collect(test_filename, \"Bob\")\n",
    "        bob_preds = self.model.predict([x_test_words, x_test_labels], batch_size=self.batch_size, verbose=1)\n",
    "        x_test_words, x_test_labels = collect(test_filename, \"Alice\")\n",
    "        alice_preds = self.model.predict([x_test_words, x_test_labels], batch_size=self.batch_size, verbose=1)\n",
    "        submission = pd.DataFrame({'dialogId': ids, 'Alice': alice_preds.ravel(), 'Bob': bob_preds.ravel()})\n",
    "        submission.to_csv(os.path.join(os.getcwd(), 'submitions', 'answerAB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 100, 300)      30000600    input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 100, 1)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 100, 301)      0           embedding_2[0][0]                \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 512)           1142784     concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 512)           0           bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           65664       dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 128)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             129         dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 31,209,177\n",
      "Trainable params: 1,208,577\n",
      "Non-trainable params: 30,000,600\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Big epoch:  0 0\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 253s - loss: 0.6848 - acc: 0.5528   \n",
      "Big epoch:  0 1\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 250s - loss: 0.6668 - acc: 0.5912   \n",
      "Big epoch:  0 2\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 251s - loss: 0.6555 - acc: 0.6088   \n",
      "Big epoch:  0 3\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 250s - loss: 0.6447 - acc: 0.6219   \n",
      "Big epoch:  0 4\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 251s - loss: 0.6437 - acc: 0.6225   \n",
      "Big epoch:  0 5\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 251s - loss: 0.6400 - acc: 0.6278   \n",
      "Big epoch:  0 6\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 251s - loss: 0.6378 - acc: 0.6306   \n",
      "Big epoch:  0 7\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 251s - loss: 0.6343 - acc: 0.6321   \n",
      "Big epoch:  0 8\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 250s - loss: 0.6334 - acc: 0.6321   \n",
      "Big epoch:  0 9\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 248s - loss: 0.6314 - acc: 0.6354   \n",
      "Big epoch:  0 10\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 248s - loss: 0.6299 - acc: 0.6348   \n",
      "Big epoch:  0 11\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 247s - loss: 0.6282 - acc: 0.6370   \n",
      "Big epoch:  0 12\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6304 - acc: 0.6333   \n",
      "Big epoch:  0 13\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6292 - acc: 0.6352   \n",
      "Big epoch:  0 14\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6274 - acc: 0.6373   \n",
      "Big epoch:  0 15\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6264 - acc: 0.6398   \n",
      "Big epoch:  0 16\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6267 - acc: 0.6378   \n",
      "Big epoch:  0 17\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6265 - acc: 0.6378   \n",
      "Big epoch:  0 18\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6243 - acc: 0.6403   \n",
      "Big epoch:  0 19\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6267 - acc: 0.6385   \n",
      "Big epoch:  0 20\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6245 - acc: 0.6375   \n",
      "Big epoch:  0 21\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6239 - acc: 0.6390   \n",
      "Big epoch:  0 22\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6249 - acc: 0.6385   \n",
      "Big epoch:  0 23\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6244 - acc: 0.6392   \n",
      "Big epoch:  0 24\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6236 - acc: 0.6401   \n",
      "Big epoch:  0 25\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6229 - acc: 0.6400   \n",
      "Big epoch:  0 26\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6202 - acc: 0.6451   \n",
      "Big epoch:  0 27\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6221 - acc: 0.6429   \n",
      "Big epoch:  0 28\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6224 - acc: 0.6407   \n",
      "Big epoch:  0 29\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6221 - acc: 0.6415   \n",
      "Big epoch:  0 30\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6223 - acc: 0.6417   \n",
      "Big epoch:  0 31\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6219 - acc: 0.6403   \n",
      "Big epoch:  0 32\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6230 - acc: 0.6399   \n",
      "Big epoch:  0 33\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6223 - acc: 0.6398   \n",
      "Big epoch:  0 34\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6205 - acc: 0.6419   \n",
      "Big epoch:  0 35\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6201 - acc: 0.6442   \n",
      "Big epoch:  0 36\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6203 - acc: 0.6451   \n",
      "Big epoch:  0 37\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6224 - acc: 0.6412   \n",
      "Big epoch:  0 38\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6204 - acc: 0.6420   \n",
      "Big epoch:  0 39\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6200 - acc: 0.6427   \n",
      "Big epoch:  0 40\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6203 - acc: 0.6430   \n",
      "Big epoch:  0 41\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6227 - acc: 0.6420   \n",
      "Big epoch:  0 42\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6221 - acc: 0.6422   \n",
      "Big epoch:  0 43\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6192 - acc: 0.6423   \n",
      "Big epoch:  0 44\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6213 - acc: 0.6419   \n",
      "Big epoch:  0 45\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6224 - acc: 0.6397   \n",
      "Big epoch:  0 46\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6203 - acc: 0.6422   \n",
      "Big epoch:  0 47\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6194 - acc: 0.6436   \n",
      "Big epoch:  0 48\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6206 - acc: 0.6411   \n",
      "Big epoch:  0 49\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6184 - acc: 0.6453   \n",
      "Big epoch:  0 50\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6195 - acc: 0.6426   \n",
      "Big epoch:  0 51\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 243s - loss: 0.6197 - acc: 0.6437   \n",
      "Big epoch:  0 52\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6188 - acc: 0.6454   \n",
      "Big epoch:  0 53\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6207 - acc: 0.6419   \n",
      "Big epoch:  0 54\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6225 - acc: 0.6387   \n",
      "Big epoch:  0 55\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6206 - acc: 0.6430   \n",
      "Big epoch:  0 56\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6200 - acc: 0.6413   \n",
      "Big epoch:  0 57\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 244s - loss: 0.6190 - acc: 0.6434   \n",
      "Big epoch:  0 58\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6205 - acc: 0.6416   \n",
      "Big epoch:  0 59\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6197 - acc: 0.6425   \n",
      "Big epoch:  0 60\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 244s - loss: 0.6201 - acc: 0.6428   \n",
      "Big epoch:  0 61\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6188 - acc: 0.6437   \n",
      "Big epoch:  0 62\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6188 - acc: 0.6439   \n",
      "Big epoch:  0 63\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6208 - acc: 0.6408   \n",
      "Big epoch:  0 64\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6197 - acc: 0.6449   \n",
      "Big epoch:  0 65\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6193 - acc: 0.6441   \n",
      "Big epoch:  0 66\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6202 - acc: 0.6429   \n",
      "Big epoch:  0 67\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6168 - acc: 0.6450   \n",
      "Big epoch:  0 68\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6185 - acc: 0.6431   \n",
      "Big epoch:  0 69\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6188 - acc: 0.6428   \n",
      "Big epoch:  0 70\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6184 - acc: 0.6436   \n",
      "Big epoch:  0 71\n",
      "Epoch 1/1\n",
      "90976/90976 [==============================] - 221s - loss: 0.6197 - acc: 0.6413   \n",
      "524342/524342 [==============================] - 425s   \n",
      "Big epoch:  1 0\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6187 - acc: 0.6464   \n",
      "Big epoch:  1 1\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6196 - acc: 0.6424   \n",
      "Big epoch:  1 2\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6203 - acc: 0.6420   \n",
      "Big epoch:  1 3\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6167 - acc: 0.6459   \n",
      "Big epoch:  1 4\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6204 - acc: 0.6417   \n",
      "Big epoch:  1 5\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6181 - acc: 0.6463   \n",
      "Big epoch:  1 6\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6181 - acc: 0.6449   \n",
      "Big epoch:  1 7\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6183 - acc: 0.6448   \n",
      "Big epoch:  1 8\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6196 - acc: 0.6436   \n",
      "Big epoch:  1 9\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6176 - acc: 0.6468   \n",
      "Big epoch:  1 10\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6181 - acc: 0.6448   \n",
      "Big epoch:  1 11\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6164 - acc: 0.6473   \n",
      "Big epoch:  1 12\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6212 - acc: 0.6407   \n",
      "Big epoch:  1 13\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6199 - acc: 0.6422   \n",
      "Big epoch:  1 14\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6179 - acc: 0.6462   \n",
      "Big epoch:  1 15\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6179 - acc: 0.6470   \n",
      "Big epoch:  1 16\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6182 - acc: 0.6443   \n",
      "Big epoch:  1 17\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6188 - acc: 0.6441   \n",
      "Big epoch:  1 18\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6174 - acc: 0.6463   \n",
      "Big epoch:  1 19\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6201 - acc: 0.6421   \n",
      "Big epoch:  1 20\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6171 - acc: 0.6436   \n",
      "Big epoch:  1 21\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6170 - acc: 0.6451   \n",
      "Big epoch:  1 22\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6223 - acc: 0.6421   \n",
      "Big epoch:  1 23\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6224 - acc: 0.6425   \n",
      "Big epoch:  1 24\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6228 - acc: 0.6424   \n",
      "Big epoch:  1 25\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6219 - acc: 0.6426   \n",
      "Big epoch:  1 26\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6197 - acc: 0.6461   \n",
      "Big epoch:  1 27\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6230 - acc: 0.6429   \n",
      "Big epoch:  1 28\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6203 - acc: 0.6444   \n",
      "Big epoch:  1 29\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6209 - acc: 0.6433   \n",
      "Big epoch:  1 30\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6207 - acc: 0.6448   \n",
      "Big epoch:  1 31\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6208 - acc: 0.6433   \n",
      "Big epoch:  1 32\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 241s - loss: 0.6222 - acc: 0.6415   \n",
      "Big epoch:  1 33\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6206 - acc: 0.6426   \n",
      "Big epoch:  1 34\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 241s - loss: 0.6194 - acc: 0.6437   \n",
      "Big epoch:  1 35\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6179 - acc: 0.6463   \n",
      "Big epoch:  1 36\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6189 - acc: 0.6466   \n",
      "Big epoch:  1 37\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 241s - loss: 0.6203 - acc: 0.6430   \n",
      "Big epoch:  1 38\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6180 - acc: 0.6462   \n",
      "Big epoch:  1 39\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6196 - acc: 0.6427   \n",
      "Big epoch:  1 40\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6190 - acc: 0.6442   \n",
      "Big epoch:  1 41\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 244s - loss: 0.6184 - acc: 0.6455   \n",
      "Big epoch:  1 42\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6188 - acc: 0.6458   \n",
      "Big epoch:  1 43\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6173 - acc: 0.6447   \n",
      "Big epoch:  1 44\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6189 - acc: 0.6443   \n",
      "Big epoch:  1 45\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6197 - acc: 0.6429   \n",
      "Big epoch:  1 46\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6180 - acc: 0.6451   \n",
      "Big epoch:  1 47\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6164 - acc: 0.6466   \n",
      "Big epoch:  1 48\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6200 - acc: 0.6429   \n",
      "Big epoch:  1 49\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6179 - acc: 0.6452   \n",
      "Big epoch:  1 50\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6183 - acc: 0.6433   \n",
      "Big epoch:  1 51\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 242s - loss: 0.6178 - acc: 0.6456   \n",
      "Big epoch:  1 52\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6178 - acc: 0.6450   \n",
      "Big epoch:  1 53\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6202 - acc: 0.6433   \n",
      "Big epoch:  1 54\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6190 - acc: 0.6433   \n",
      "Big epoch:  1 55\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6195 - acc: 0.6450   \n",
      "Big epoch:  1 56\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6215 - acc: 0.6396   \n",
      "Big epoch:  1 57\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6203 - acc: 0.6427   \n",
      "Big epoch:  1 58\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6216 - acc: 0.6413   \n",
      "Big epoch:  1 59\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6201 - acc: 0.6414   \n",
      "Big epoch:  1 60\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6211 - acc: 0.6428   \n",
      "Big epoch:  1 61\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6189 - acc: 0.6445   \n",
      "Big epoch:  1 62\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6246 - acc: 0.6397   \n",
      "Big epoch:  1 63\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6291 - acc: 0.6347   \n",
      "Big epoch:  1 64\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6259 - acc: 0.6399   \n",
      "Big epoch:  1 65\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6235 - acc: 0.6411   \n",
      "Big epoch:  1 66\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6253 - acc: 0.6390   \n",
      "Big epoch:  1 67\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 242s - loss: 0.6195 - acc: 0.6436   \n",
      "Big epoch:  1 68\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6212 - acc: 0.6427   \n",
      "Big epoch:  1 69\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6213 - acc: 0.6427   \n",
      "Big epoch:  1 70\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 243s - loss: 0.6211 - acc: 0.6413   \n",
      "Big epoch:  1 71\n",
      "Epoch 1/1\n",
      "90976/90976 [==============================] - 221s - loss: 0.6216 - acc: 0.6410   \n",
      "524342/524342 [==============================] - 425s   \n",
      "Big epoch:  2 0\n",
      "Epoch 1/1\n",
      " 44032/100000 [============>.................] - ETA: 135s - loss: 0.6203 - acc: 0.6446"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-572ecda6c9ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTuringRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_FILENAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_FILENAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-062fd54eb52c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_filename, val_filename, vocabulary)\u001b[0m\n\u001b[1;32m     49\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                verbose=1)\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "rnn = TuringRNN(batch_size=256)\n",
    "rnn.build(vocabulary, weights)\n",
    "rnn.train(TRAIN_FILENAME, VAL_FILENAME, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = TuringRNN(batch_size=256)\n",
    "rnn.load(\"models/LSTM256_Emb300_Dense128_dropout0.3.h5\")\n",
    "rnn.predict(TEST_FILENAME, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class Dialog:\n",
    "    def __init__(self, dialog_id, context, first_user_id, second_user_id,\n",
    "                 first_user_is_bot=None, second_user_is_bot=None):\n",
    "        self.dialog_id = int(dialog_id)\n",
    "        self.first_user_id = first_user_id\n",
    "        self.second_user_id = second_user_id\n",
    "        self.context = context\n",
    "        self.messages = []\n",
    "        self.first_user_is_bot = first_user_is_bot\n",
    "        self.second_user_is_bot = second_user_is_bot\n",
    "\n",
    "    def add_message(self, user_id, text):\n",
    "        Message = namedtuple(\"Message\", \"user_id text\")\n",
    "        self.messages.append(Message(user_id, text))\n",
    "\n",
    "    def get_first_user_messages(self):\n",
    "        return [message.text for message in self.messages if message.user_id == self.first_user_id]\n",
    "\n",
    "    def get_second_user_messages(self):\n",
    "        return [message.text for message in self.messages if message.user_id == self.second_user_id]\n",
    "\n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "\n",
    "    def get_context(self):\n",
    "        return self.context\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.dialog_id) + \" \" + self.first_user_id + \" \" + self.second_user_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def parse(filename, get_df=True):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        dialogs = []\n",
    "        if text[0] == \"[\":\n",
    "            text = '{\"dialogs\": ' + text + '}'\n",
    "            dialogs = json.loads(text)[\"dialogs\"]\n",
    "        else:\n",
    "            dialogs.append(json.loads(text))\n",
    "\n",
    "        result = []\n",
    "        for dialog in dialogs:\n",
    "            users = dialog[\"users\"]\n",
    "            messages = dialog[\"thread\"]\n",
    "            first_user = users[0]\n",
    "            second_user = users[1]\n",
    "            first_user_is_bot = None\n",
    "            second_user_is_bot = None\n",
    "            if \"userType\" in first_user:\n",
    "                first_user_is_bot = first_user[\"userType\"] != \"Human\"\n",
    "                second_user_is_bot = second_user[\"userType\"] != \"Human\"\n",
    "\n",
    "            dialog = Dialog(dialog_id=dialog[\"dialogId\"], context=dialog[\"context\"],\n",
    "                            first_user_id=first_user[\"id\"], second_user_id=second_user[\"id\"],\n",
    "                            first_user_is_bot=first_user_is_bot, second_user_is_bot=second_user_is_bot)\n",
    "            for message in messages:\n",
    "                dialog.add_message(message[\"userId\"], message[\"text\"])\n",
    "            result.append(dialog)\n",
    "        if get_df:\n",
    "            df = pd.DataFrame()\n",
    "            df[\"dialogId\"] = [dialog.dialog_id for dialog in result]\n",
    "            df[\"context\"] = [dialog.context for dialog in result]\n",
    "            df[\"messages\"] = [[message.text for message in dialog.messages] for dialog in result]\n",
    "            df[\"message_users\"] = [[message.user_id for message in dialog.messages] for dialog in result]\n",
    "            if result[0].first_user_is_bot is not None:\n",
    "                bob_is_bot = [bool(dialog.first_user_is_bot) and dialog.first_user_id == \"Bob\" or\n",
    "                              bool(dialog.second_user_is_bot) and dialog.second_user_id == \"Bob\" for dialog in result]\n",
    "                alice_is_bot = [bool(dialog.first_user_is_bot) and dialog.first_user_id == \"Alice\" or\n",
    "                                bool(dialog.second_user_is_bot) and dialog.second_user_id == \"Alice\" for dialog in result]\n",
    "                df[\"BobIsBot\"] = bob_is_bot\n",
    "                df[\"AliceIsBot\"] = alice_is_bot\n",
    "            return df\n",
    "        return result\n",
    "\n",
    "def collect(json_filename, user_id, maxlen=100):\n",
    "    dialogs = parse(json_filename)\n",
    "    data = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    for dialog in dialogs:\n",
    "        word_indices = []\n",
    "        speaker_labels = []\n",
    "        texts = [message.text for message in dialog.messages]\n",
    "        for text in texts:\n",
    "            word_indices += text_to_indices(text, vocabulary)\n",
    "        speaker_labels += [message.user_id == user_id for message in dialog.messages]\n",
    "        word_indices = np.array(word_indices, dtype=\"int32\")\n",
    "        speaker_labels = np.array(speaker_labels, dtype=\"float32\")\n",
    "        data.append(word_indices)\n",
    "        labels.append(speaker_labels)\n",
    "    data = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "    labels = sequence.pad_sequences(labels, maxlen=maxlen)\n",
    "    labels = labels.reshape(labels.shape[0], labels.shape[1], 1)\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = TuringRNN(batch_size=256)\n",
    "rnn.load(\"models/LSTM256_Emb300_Dense128_dropout0.3.h5\")\n",
    "rnn.predict_new(collect, \"data/day-1.json\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698646084415\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "def evaluate(json_filename, submition):\n",
    "    dialogs_df = parse(json_filename)\n",
    "    submission = pd.read_csv(submition)\n",
    "    bob_true = dialogs_df[\"BobIsBot\"].apply(lambda x: float(not x)).tolist()\n",
    "    alice_true = dialogs_df[\"AliceIsBot\"].apply(lambda x: float(not x)).tolist()\n",
    "    bob_pred = submission[\"Bob\"].tolist()\n",
    "    alice_pred = submission[\"Alice\"].tolist()\n",
    "    true = bob_true + alice_true\n",
    "    pred = bob_pred + alice_pred\n",
    "    print(roc_auc_score(true, pred))\n",
    "evaluate(\"data/day-1.json\", os.path.join(os.getcwd(), 'submitions', 'answerAB.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}